"""
Visualization Script for Model Evaluation Results

This script loads the CSV evaluation results and creates comprehensive
visualizations for model comparison and analysis.
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import sys
import os

# Set plotting style
plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')
sns.set_palette("husl")


def load_evaluation_results(results_dir: str = "evaluation_results") -> tuple:
    """
    Load evaluation results and metric descriptions.
    
    Args:
        results_dir: Directory containing evaluation results
        
    Returns:
        Tuple of (results_df, descriptions_df)
    """
    results_path = os.path.join(results_dir, 'model_comparison_results.csv')
    descriptions_path = os.path.join(results_dir, 'metric_descriptions.csv')
    
    if not os.path.exists(results_path):
        raise FileNotFoundError(f"Results file not found: {results_path}")
    
    results_df = pd.read_csv(results_path)
    
    descriptions_df = None
    if os.path.exists(descriptions_path):
        descriptions_df = pd.read_csv(descriptions_path)
    
    return results_df, descriptions_df


def create_comprehensive_visualization(results_df: pd.DataFrame, save_dir: str = "evaluation_results"):
    """Create comprehensive visualization of evaluation results."""
    print("Creating comprehensive evaluation visualization...")
    
    # Filter successful evaluations
    successful_df = results_df[~results_df.get('evaluation_error', pd.Series()).notna()]
    
    if successful_df.empty:
        print("âŒ No successful evaluations to visualize")
        return
    
    # Create main comparison figure
    fig, axes = plt.subplots(3, 3, figsize=(18, 15))
    
    models = successful_df['model_name'].tolist()
    colors = sns.color_palette("husl", len(models))
    
    # 1. RMSE Comparison
    ax = axes[0, 0]
    rmse_values = successful_df['rmse'].tolist()
    bars = ax.bar(models, rmse_values, color=colors, alpha=0.7)
    ax.set_ylabel('RMSE')
    ax.set_title('Root Mean Square Error\n(Lower is Better)')
    ax.grid(True, alpha=0.3)
    
    # Add value labels
    for bar, value in zip(bars, rmse_values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
    
    # 2. Distributional Similarity (KS Test)
    ax = axes[0, 1]
    ks_values = successful_df['ks_statistic'].tolist()
    bars = ax.bar(models, ks_values, color=colors, alpha=0.7)
    ax.set_ylabel('KS Statistic')
    ax.set_title('Distribution Similarity\n(Lower is Better)')
    ax.grid(True, alpha=0.3)
    
    for bar, value in zip(bars, ks_values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
    
    # 3. Path Correlation
    ax = axes[0, 2]
    corr_values = successful_df['mean_path_correlation'].tolist()
    bars = ax.bar(models, corr_values, color=colors, alpha=0.7)
    ax.set_ylabel('Mean Path Correlation')
    ax.set_title('Path-wise Correlation\n(Higher is Better)')
    ax.grid(True, alpha=0.3)
    
    for bar, value in zip(bars, corr_values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
    
    # 4. Wasserstein Distance
    ax = axes[1, 0]
    wass_values = successful_df['wasserstein_distance'].tolist()
    bars = ax.bar(models, wass_values, color=colors, alpha=0.7)
    ax.set_ylabel('Wasserstein Distance')
    ax.set_title('Distribution Distance\n(Lower is Better)')
    ax.grid(True, alpha=0.3)
    
    for bar, value in zip(bars, wass_values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
    
    # 5. Computational Performance
    ax = axes[1, 1]
    time_values = successful_df['mean_forward_time_ms'].tolist()
    param_values = successful_df['total_parameters'].tolist()
    
    # Scatter plot: parameters vs time
    scatter = ax.scatter(param_values, time_values, c=range(len(models)), 
                        cmap='viridis', s=100, alpha=0.7)
    
    for i, model in enumerate(models):
        ax.annotate(model.split('_')[0], (param_values[i], time_values[i]), 
                   xytext=(5, 5), textcoords='offset points', fontsize=10)
    
    ax.set_xlabel('Total Parameters')
    ax.set_ylabel('Forward Time (ms)')
    ax.set_title('Computational Efficiency')
    ax.grid(True, alpha=0.3)
    
    # 6. Statistical Moments Comparison
    ax = axes[1, 2]
    
    # Compare means and stds
    gen_means = successful_df['generated_mean'].tolist()
    gt_means = successful_df['ground_truth_mean'].tolist()
    gen_stds = successful_df['generated_std'].tolist()
    gt_stds = successful_df['ground_truth_std'].tolist()
    
    x = np.arange(len(models))
    width = 0.2
    
    ax.bar(x - width*1.5, gen_means, width, label='Gen Mean', alpha=0.8)
    ax.bar(x - width*0.5, gt_means, width, label='GT Mean', alpha=0.8)
    ax.bar(x + width*0.5, gen_stds, width, label='Gen Std', alpha=0.8)
    ax.bar(x + width*1.5, gt_stds, width, label='GT Std', alpha=0.8)
    
    ax.set_xlabel('Model')
    ax.set_ylabel('Value')
    ax.set_title('Statistical Moments Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels([m.split('_')[0] for m in models])
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 7. Error Analysis
    ax = axes[2, 0]
    
    mean_errors = successful_df['mean_relative_error'].tolist()
    std_errors = successful_df['std_relative_error'].tolist()
    
    x = np.arange(len(models))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, mean_errors, width, label='Mean Error', alpha=0.8)
    bars2 = ax.bar(x + width/2, std_errors, width, label='Std Error', alpha=0.8)
    
    ax.set_xlabel('Model')
    ax.set_ylabel('Relative Error')
    ax.set_title('Moment Matching Errors\n(Lower is Better)')
    ax.set_xticks(x)
    ax.set_xticklabels([m.split('_')[0] for m in models])
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 8. Quality Metrics Radar Chart
    ax = axes[2, 1]
    
    # Normalize metrics for radar chart (0-1 scale)
    quality_metrics = ['rmse', 'ks_statistic', 'wasserstein_distance', 'mean_path_correlation']
    
    if len(models) >= 1:
        # For first model (can extend to multiple models)
        model_data = successful_df.iloc[0]
        
        # Normalize metrics (invert for metrics where lower is better)
        normalized_values = []
        metric_labels = []
        
        for metric in quality_metrics:
            if metric in successful_df.columns:
                value = model_data[metric]
                
                if metric in ['rmse', 'ks_statistic', 'wasserstein_distance']:
                    # Lower is better - invert and normalize
                    max_val = successful_df[metric].max()
                    normalized_val = 1 - (value / (max_val + 1e-8))
                else:
                    # Higher is better - normalize directly
                    max_val = successful_df[metric].max()
                    min_val = successful_df[metric].min()
                    normalized_val = (value - min_val) / (max_val - min_val + 1e-8)
                
                normalized_values.append(max(0, min(1, normalized_val)))
                metric_labels.append(metric.replace('_', ' ').title())
        
        # Create radar chart
        if len(normalized_values) > 0:
            angles = np.linspace(0, 2*np.pi, len(normalized_values), endpoint=False)
            normalized_values += normalized_values[:1]  # Complete the circle
            angles = np.concatenate((angles, [angles[0]]))
            
            ax.plot(angles, normalized_values, 'o-', linewidth=2, label=models[0].split('_')[0])
            ax.fill(angles, normalized_values, alpha=0.25)
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(metric_labels)
            ax.set_ylim(0, 1)
            ax.set_title('Quality Profile\n(Outer is Better)')
            ax.grid(True)
    
    # 9. Summary Table
    ax = axes[2, 2]
    ax.axis('off')
    
    # Create summary text
    summary_lines = [
        "Evaluation Summary",
        "=" * 18,
        "",
        f"Models Evaluated: {len(successful_df)}",
        f"Dataset: Ornstein-Uhlenbeck",
        f"Samples: 32 per model",
        "",
        "Key Findings:"
    ]
    
    if len(successful_df) > 1:
        # Compare A1 vs A2
        a1_data = successful_df[successful_df['model_name'].str.contains('A1')].iloc[0] if any(successful_df['model_name'].str.contains('A1')) else None
        a2_data = successful_df[successful_df['model_name'].str.contains('A2')].iloc[0] if any(successful_df['model_name'].str.contains('A2')) else None
        
        if a1_data is not None and a2_data is not None:
            a1_rmse = a1_data['rmse']
            a2_rmse = a2_data['rmse']
            
            summary_lines.extend([
                f"â€¢ Same RMSE: {a1_rmse:.4f}",
                f"â€¢ Same architecture (199 params)",
                f"â€¢ Different loss functions",
                f"â€¢ Similar performance overall"
            ])
    
    summary_lines.extend([
        "",
        "Status: âœ… SUCCESSFUL",
        "Ready for further analysis"
    ])
    
    summary_text = "\n".join(summary_lines)
    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=11,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgreen', alpha=0.1))
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'comprehensive_evaluation_visualization.png'), 
                dpi=300, bbox_inches='tight')
    print(f"Comprehensive visualization saved to: {save_dir}/comprehensive_evaluation_visualization.png")
    plt.close()


def analyze_evaluation_results(results_dir: str = "evaluation_results"):
    """Analyze and visualize evaluation results."""
    print("Analyzing Model Evaluation Results")
    print("=" * 50)
    
    try:
        # Load results
        results_df, descriptions_df = load_evaluation_results(results_dir)
        
        print(f"Loaded results: {len(results_df)} models")
        
        if results_df.empty:
            print("âŒ No results to analyze")
            return False
        
        # Print detailed analysis
        print(f"\nDetailed Analysis:")
        print("-" * 20)
        
        for idx, row in results_df.iterrows():
            model_name = row['model_name']
            print(f"\n{model_name}:")
            
            if 'evaluation_error' in row and pd.notna(row['evaluation_error']):
                print(f"  âŒ Evaluation failed: {row['evaluation_error']}")
                continue
            
            # Key metrics
            rmse = row.get('rmse', float('nan'))
            ks_stat = row.get('ks_statistic', float('nan'))
            correlation = row.get('mean_path_correlation', float('nan'))
            params = row.get('total_parameters', 0)
            time_ms = row.get('mean_forward_time_ms', float('nan'))
            
            print(f"  RMSE: {rmse:.4f}")
            print(f"  KS Statistic: {ks_stat:.4f} (p={row.get('ks_p_value', 0):.2e})")
            print(f"  Path Correlation: {correlation:.4f}")
            print(f"  Parameters: {params:,}")
            print(f"  Forward Time: {time_ms:.2f}ms")
            
            # Distribution comparison
            gen_mean = row.get('generated_mean', float('nan'))
            gt_mean = row.get('ground_truth_mean', float('nan'))
            gen_std = row.get('generated_std', float('nan'))
            gt_std = row.get('ground_truth_std', float('nan'))
            
            print(f"  Generated: Î¼={gen_mean:.4f}, Ïƒ={gen_std:.4f}")
            print(f"  Ground Truth: Î¼={gt_mean:.4f}, Ïƒ={gt_std:.4f}")
        
        # Create visualizations
        create_comprehensive_visualization(results_df, results_dir)
        
        # Print comparison summary
        print(f"\n" + "="*50)
        print("COMPARISON SUMMARY")
        print("="*50)
        
        successful_df = results_df[~results_df.get('evaluation_error', pd.Series()).notna()]
        
        if len(successful_df) > 1:
            print(f"Model Performance Ranking:")
            
            # Rank by RMSE
            rmse_ranking = successful_df.nsmallest(10, 'rmse')
            print(f"\nBy RMSE (lower is better):")
            for i, (idx, row) in enumerate(rmse_ranking.iterrows(), 1):
                print(f"  {i}. {row['model_name']}: {row['rmse']:.4f}")
            
            # Rank by correlation
            corr_ranking = successful_df.nlargest(10, 'mean_path_correlation')
            print(f"\nBy Path Correlation (higher is better):")
            for i, (idx, row) in enumerate(corr_ranking.iterrows(), 1):
                print(f"  {i}. {row['model_name']}: {row['mean_path_correlation']:.4f}")
            
            # Statistical significance of differences
            if len(successful_df) == 2:
                model1 = successful_df.iloc[0]
                model2 = successful_df.iloc[1]
                
                print(f"\nA1 vs A2 Detailed Comparison:")
                print(f"  RMSE: {model1['rmse']:.6f} vs {model2['rmse']:.6f}")
                print(f"  KS Statistic: {model1['ks_statistic']:.6f} vs {model2['ks_statistic']:.6f}")
                print(f"  Correlation: {model1['mean_path_correlation']:.6f} vs {model2['mean_path_correlation']:.6f}")
                print(f"  Speed: {model1['mean_forward_time_ms']:.2f}ms vs {model2['mean_forward_time_ms']:.2f}ms")
                
                # Check if results are identical (expected for same generator)
                rmse_diff = abs(model1['rmse'] - model2['rmse'])
                if rmse_diff < 1e-6:
                    print(f"  âœ… Identical RMSE (expected - same generator)")
                else:
                    print(f"  âš ï¸ Different RMSE ({rmse_diff:.6f} difference)")
        
        print(f"\nâœ… Analysis complete!")
        print(f"   Visualization saved to: {results_dir}/comprehensive_evaluation_visualization.png")
        
        return True
        
    except Exception as e:
        print(f"âŒ Analysis failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def create_csv_summary(results_dir: str = "evaluation_results"):
    """Create a clean CSV summary for easy analysis."""
    try:
        results_df, _ = load_evaluation_results(results_dir)
        
        if results_df.empty:
            return
        
        # Create clean summary with key metrics
        successful_df = results_df[~results_df.get('evaluation_error', pd.Series()).notna()]
        
        summary_columns = [
            'model_name', 'experiment_id', 'generator_type', 'loss_type',
            'rmse', 'ks_statistic', 'ks_p_value', 'wasserstein_distance',
            'mean_path_correlation', 'generated_mean', 'generated_std',
            'total_parameters', 'mean_forward_time_ms'
        ]
        
        # Select available columns
        available_columns = [col for col in summary_columns if col in successful_df.columns]
        clean_summary = successful_df[available_columns].copy()
        
        # Round numerical columns
        numerical_columns = clean_summary.select_dtypes(include=[np.number]).columns
        clean_summary[numerical_columns] = clean_summary[numerical_columns].round(6)
        
        # Save clean summary
        summary_path = os.path.join(results_dir, 'clean_model_summary.csv')
        clean_summary.to_csv(summary_path, index=False)
        
        print(f"âœ… Clean summary saved to: {summary_path}")
        
        return clean_summary
        
    except Exception as e:
        print(f"âŒ Failed to create summary: {e}")
        return None


def main():
    """Main analysis function."""
    print("Model Evaluation Results Analysis")
    print("=" * 60)
    
    results_dir = "evaluation_results"
    
    # Check if results exist
    results_path = os.path.join(results_dir, 'model_comparison_results.csv')
    if not os.path.exists(results_path):
        print(f"âŒ No evaluation results found at {results_path}")
        print(f"   Run simple_model_evaluation.py first to generate results")
        return False
    
    # Analyze results
    success = analyze_evaluation_results(results_dir)
    
    if success:
        # Create clean summary
        clean_summary = create_csv_summary(results_dir)
        
        print(f"\nðŸŽ‰ ANALYSIS COMPLETE!")
        print(f"   Comprehensive visualization created")
        print(f"   Clean summary CSV generated")
        print(f"   Ready for further research analysis")
        
        if clean_summary is not None:
            print(f"\nQuick Summary:")
            print(clean_summary.to_string(index=False))
        
        return True
    else:
        print(f"\nâŒ Analysis failed")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
